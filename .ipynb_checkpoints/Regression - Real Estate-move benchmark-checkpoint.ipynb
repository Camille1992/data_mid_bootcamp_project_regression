{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3a6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from numpy import where\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.special import inv_boxcox   \n",
    "import os #we will use the function listdir to list files in a folder\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, grangercausalitytests\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf,month_plot,quarter_plot\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eada02",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf9c8d",
   "metadata": {},
   "source": [
    "## Data collection and discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63234f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Data Exploration\n",
    "\n",
    "# Renaming columns\n",
    "def rename_columns(df):\n",
    "    for i in range(0, df.shape[1]):\n",
    "        df.rename(columns={df.columns[i]: column_names[i]}, inplace=True)    \n",
    "    return df\n",
    "\n",
    "# Checking the NaNs\n",
    "def check_nan(df):\n",
    "    nulls = pd.DataFrame(df.isna().sum()/len(df))\n",
    "    nulls= nulls.reset_index()\n",
    "    nulls.columns = ['column_name', 'Percentage Null Values']\n",
    "    nulls.sort_values(by='Percentage Null Values', ascending = False)\n",
    "    return nulls\n",
    "\n",
    "# Checking the value counts\n",
    "def check_count(df):\n",
    "    value_counts = []\n",
    "    for column in df.columns:\n",
    "        x = len(df[column].value_counts())\n",
    "        value_counts.append(x)\n",
    "    counts = pd.DataFrame(value_counts)\n",
    "    counts = counts.reset_index()\n",
    "    counts.columns = ['name', 'value_counts']\n",
    "    for i in range(0, df.shape[1]):\n",
    "        counts['name'] = counts['name'].replace(i,df.columns[i])\n",
    "    return counts\n",
    "\n",
    "# Checking for constants\n",
    "def get_constant(df):\n",
    "    to_remove = []\n",
    "    for col in df.columns:\n",
    "        maximum = max(list(df[col].value_counts()))\n",
    "        percent = round(maximum/len(df)*100,2)\n",
    "        if percent > 75:\n",
    "            to_remove.append(col)\n",
    "            print(col)\n",
    "            print(maximum)\n",
    "            print(percent, '%')\n",
    "            print()\n",
    "    print(to_remove)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949e07c",
   "metadata": {},
   "source": [
    "##### Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20edd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21597, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>10/13/14</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>12/9/14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2/25/15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>12/9/14</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2/18/15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1   2     3     4      5    6   7   8   9   10    11  \\\n",
       "0  7129300520  10/13/14   3  1.00  1180   5650  1.0   0   0   3   7  1180   \n",
       "1  6414100192   12/9/14   3  2.25  2570   7242  2.0   0   0   3   7  2170   \n",
       "2  5631500400   2/25/15   2  1.00   770  10000  1.0   0   0   3   6   770   \n",
       "3  2487200875   12/9/14   4  3.00  1960   5000  1.0   0   0   5   7  1050   \n",
       "4  1954400510   2/18/15   3  2.00  1680   8080  1.0   0   0   3   8  1680   \n",
       "\n",
       "    12    13    14     15       16       17    18    19      20  \n",
       "0    0  1955     0  98178  47.5112 -122.257  1340  5650  221900  \n",
       "1  400  1951  1991  98125  47.7210 -122.319  1690  7639  538000  \n",
       "2    0  1933     0  98028  47.7379 -122.233  2720  8062  180000  \n",
       "3  910  1965     0  98136  47.5208 -122.393  1360  5000  604000  \n",
       "4    0  1987     0  98074  47.6168 -122.045  1800  7503  510000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('regression_data.csv', header=None)\n",
    "# we create a copy that we will use to train the model and have a reference point\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f7ad6",
   "metadata": {},
   "source": [
    "##### First, let's rename the columns to make the exploration more efficient\n",
    "We'll also set the id as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66773d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7129300520</th>\n",
       "      <td>10/13/14</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414100192</th>\n",
       "      <td>12/9/14</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631500400</th>\n",
       "      <td>2/25/15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487200875</th>\n",
       "      <td>12/9/14</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954400510</th>\n",
       "      <td>2/18/15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "id                                                                         \n",
       "7129300520  10/13/14         3       1.00         1180      5650     1.0   \n",
       "6414100192   12/9/14         3       2.25         2570      7242     2.0   \n",
       "5631500400   2/25/15         2       1.00          770     10000     1.0   \n",
       "2487200875   12/9/14         4       3.00         1960      5000     1.0   \n",
       "1954400510   2/18/15         3       2.00         1680      8080     1.0   \n",
       "\n",
       "            waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "id                                                                          \n",
       "7129300520           0     0          3      7        1180              0   \n",
       "6414100192           0     0          3      7        2170            400   \n",
       "5631500400           0     0          3      6         770              0   \n",
       "2487200875           0     0          5      7        1050            910   \n",
       "1954400510           0     0          3      8        1680              0   \n",
       "\n",
       "            yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "id                                                                             \n",
       "7129300520      1955             0    98178  47.5112 -122.257           1340   \n",
       "6414100192      1951          1991    98125  47.7210 -122.319           1690   \n",
       "5631500400      1933             0    98028  47.7379 -122.233           2720   \n",
       "2487200875      1965             0    98136  47.5208 -122.393           1360   \n",
       "1954400510      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "            sqft_lot15   price  \n",
       "id                              \n",
       "7129300520        5650  221900  \n",
       "6414100192        7639  538000  \n",
       "5631500400        8062  180000  \n",
       "2487200875        5000  604000  \n",
       "1954400510        7503  510000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['id', 'date','bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition','grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'price']\n",
    "data = rename_columns(data)\n",
    "data = data.set_index('id')\n",
    "copy = data.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11db7e8",
   "metadata": {},
   "source": [
    "### Now let's check the data types, unique values, null values and statistical overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe85d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21597 entries, 7129300520 to 1523300157\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           21597 non-null  object \n",
      " 1   bedrooms       21597 non-null  int64  \n",
      " 2   bathrooms      21597 non-null  float64\n",
      " 3   sqft_living    21597 non-null  int64  \n",
      " 4   sqft_lot       21597 non-null  int64  \n",
      " 5   floors         21597 non-null  float64\n",
      " 6   waterfront     21597 non-null  int64  \n",
      " 7   view           21597 non-null  int64  \n",
      " 8   condition      21597 non-null  int64  \n",
      " 9   grade          21597 non-null  int64  \n",
      " 10  sqft_above     21597 non-null  int64  \n",
      " 11  sqft_basement  21597 non-null  int64  \n",
      " 12  yr_built       21597 non-null  int64  \n",
      " 13  yr_renovated   21597 non-null  int64  \n",
      " 14  zipcode        21597 non-null  int64  \n",
      " 15  lat            21597 non-null  float64\n",
      " 16  long           21597 non-null  float64\n",
      " 17  sqft_living15  21597 non-null  int64  \n",
      " 18  sqft_lot15     21597 non-null  int64  \n",
      " 19  price          21597 non-null  int64  \n",
      "dtypes: float64(4), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8863a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>2.159700e+04</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>21597.000000</td>\n",
       "      <td>2.159700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.373200</td>\n",
       "      <td>2.115826</td>\n",
       "      <td>2080.321850</td>\n",
       "      <td>1.509941e+04</td>\n",
       "      <td>1.494096</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.234292</td>\n",
       "      <td>3.409825</td>\n",
       "      <td>7.657915</td>\n",
       "      <td>1788.596842</td>\n",
       "      <td>291.725008</td>\n",
       "      <td>1970.999676</td>\n",
       "      <td>84.464787</td>\n",
       "      <td>98077.951845</td>\n",
       "      <td>47.560093</td>\n",
       "      <td>-122.213982</td>\n",
       "      <td>1986.620318</td>\n",
       "      <td>12758.283512</td>\n",
       "      <td>5.402966e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.926299</td>\n",
       "      <td>0.768984</td>\n",
       "      <td>918.106125</td>\n",
       "      <td>4.141264e+04</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.086549</td>\n",
       "      <td>0.766390</td>\n",
       "      <td>0.650546</td>\n",
       "      <td>1.173200</td>\n",
       "      <td>827.759761</td>\n",
       "      <td>442.667800</td>\n",
       "      <td>29.375234</td>\n",
       "      <td>401.821438</td>\n",
       "      <td>53.513072</td>\n",
       "      <td>0.138552</td>\n",
       "      <td>0.140724</td>\n",
       "      <td>685.230472</td>\n",
       "      <td>27274.441950</td>\n",
       "      <td>3.673681e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>7.800000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.471100</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "      <td>3.220000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98065.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.231000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "      <td>4.500000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068500e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "      <td>6.450000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "      <td>7.700000e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bedrooms     bathrooms   sqft_living      sqft_lot        floors  \\\n",
       "count  21597.000000  21597.000000  21597.000000  2.159700e+04  21597.000000   \n",
       "mean       3.373200      2.115826   2080.321850  1.509941e+04      1.494096   \n",
       "std        0.926299      0.768984    918.106125  4.141264e+04      0.539683   \n",
       "min        1.000000      0.500000    370.000000  5.200000e+02      1.000000   \n",
       "25%        3.000000      1.750000   1430.000000  5.040000e+03      1.000000   \n",
       "50%        3.000000      2.250000   1910.000000  7.618000e+03      1.500000   \n",
       "75%        4.000000      2.500000   2550.000000  1.068500e+04      2.000000   \n",
       "max       33.000000      8.000000  13540.000000  1.651359e+06      3.500000   \n",
       "\n",
       "         waterfront          view     condition         grade    sqft_above  \\\n",
       "count  21597.000000  21597.000000  21597.000000  21597.000000  21597.000000   \n",
       "mean       0.007547      0.234292      3.409825      7.657915   1788.596842   \n",
       "std        0.086549      0.766390      0.650546      1.173200    827.759761   \n",
       "min        0.000000      0.000000      1.000000      3.000000    370.000000   \n",
       "25%        0.000000      0.000000      3.000000      7.000000   1190.000000   \n",
       "50%        0.000000      0.000000      3.000000      7.000000   1560.000000   \n",
       "75%        0.000000      0.000000      4.000000      8.000000   2210.000000   \n",
       "max        1.000000      4.000000      5.000000     13.000000   9410.000000   \n",
       "\n",
       "       sqft_basement      yr_built  yr_renovated       zipcode           lat  \\\n",
       "count   21597.000000  21597.000000  21597.000000  21597.000000  21597.000000   \n",
       "mean      291.725008   1970.999676     84.464787  98077.951845     47.560093   \n",
       "std       442.667800     29.375234    401.821438     53.513072      0.138552   \n",
       "min         0.000000   1900.000000      0.000000  98001.000000     47.155900   \n",
       "25%         0.000000   1951.000000      0.000000  98033.000000     47.471100   \n",
       "50%         0.000000   1975.000000      0.000000  98065.000000     47.571800   \n",
       "75%       560.000000   1997.000000      0.000000  98118.000000     47.678000   \n",
       "max      4820.000000   2015.000000   2015.000000  98199.000000     47.777600   \n",
       "\n",
       "               long  sqft_living15     sqft_lot15         price  \n",
       "count  21597.000000   21597.000000   21597.000000  2.159700e+04  \n",
       "mean    -122.213982    1986.620318   12758.283512  5.402966e+05  \n",
       "std        0.140724     685.230472   27274.441950  3.673681e+05  \n",
       "min     -122.519000     399.000000     651.000000  7.800000e+04  \n",
       "25%     -122.328000    1490.000000    5100.000000  3.220000e+05  \n",
       "50%     -122.231000    1840.000000    7620.000000  4.500000e+05  \n",
       "75%     -122.125000    2360.000000   10083.000000  6.450000e+05  \n",
       "max     -121.315000    6210.000000  871200.000000  7.700000e+06  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a66503",
   "metadata": {},
   "source": [
    "#### Checking the nulls and the value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1861c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      column_name  Percentage Null Values\n",
      "0            date                     0.0\n",
      "1        bedrooms                     0.0\n",
      "2       bathrooms                     0.0\n",
      "3     sqft_living                     0.0\n",
      "4        sqft_lot                     0.0\n",
      "5          floors                     0.0\n",
      "6      waterfront                     0.0\n",
      "7            view                     0.0\n",
      "8       condition                     0.0\n",
      "9           grade                     0.0\n",
      "10     sqft_above                     0.0\n",
      "11  sqft_basement                     0.0\n",
      "12       yr_built                     0.0\n",
      "13   yr_renovated                     0.0\n",
      "14        zipcode                     0.0\n",
      "15            lat                     0.0\n",
      "16           long                     0.0\n",
      "17  sqft_living15                     0.0\n",
      "18     sqft_lot15                     0.0\n",
      "19          price                     0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>value_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sqft_living</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sqft_lot</td>\n",
       "      <td>9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>floors</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>waterfront</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>view</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>condition</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grade</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sqft_above</td>\n",
       "      <td>942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sqft_basement</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yr_built</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>yr_renovated</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zipcode</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lat</td>\n",
       "      <td>5033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>long</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sqft_living15</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sqft_lot15</td>\n",
       "      <td>8682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>price</td>\n",
       "      <td>3622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  value_counts\n",
       "0            date           372\n",
       "1        bedrooms            12\n",
       "2       bathrooms            29\n",
       "3     sqft_living          1034\n",
       "4        sqft_lot          9776\n",
       "5          floors             6\n",
       "6      waterfront             2\n",
       "7            view             5\n",
       "8       condition             5\n",
       "9           grade            11\n",
       "10     sqft_above           942\n",
       "11  sqft_basement           306\n",
       "12       yr_built           116\n",
       "13   yr_renovated            70\n",
       "14        zipcode            70\n",
       "15            lat          5033\n",
       "16           long           751\n",
       "17  sqft_living15           777\n",
       "18     sqft_lot15          8682\n",
       "19          price          3622"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulls = check_nan(data)\n",
    "print(nulls)\n",
    "counts = check_count(data)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41af36",
   "metadata": {},
   "source": [
    "### First observations\n",
    "Almost all of the data is numerical. \\\n",
    "The qualitative categorical data has already been assigned numerical values.\\\n",
    "The features identified as categoricals are: waterfront, view, condition, grade, zipcode. \\\n",
    "The only object type we have is the `date`, `yr_built` and `yr_renovated` that I will change into datetime type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd8df4",
   "metadata": {},
   "source": [
    "## Data cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1bd3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for Data Cleaning and Feature Engineering\n",
    "\n",
    "# Log Transform\n",
    "def log_transfom_x(x):\n",
    "    if np.isfinite(x) and x!=0:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def log_transform_df(df):\n",
    "    df_log = df.copy()\n",
    "    for col in df_log.columns:\n",
    "        df_log[col] = list(map(log_transfom_x, df_log[col]))\n",
    "    for col in df_log.columns:\n",
    "        df_log[col] = df_log[col].fillna(np.mean(df_log[col]))\n",
    "        sns.distplot(df_log[col], fit=norm)\n",
    "        fig = plt.figure()\n",
    "    return df_log\n",
    "\n",
    "# Square root transform\n",
    "def sqrt_transfom_x(x):\n",
    "    if np.isfinite(x) and x>=0:\n",
    "        return np.sqrt(x)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def sqrt_transform_df(df):\n",
    "    df_log = df.copy()\n",
    "    for col in df_log.columns:\n",
    "        df_log[col] = list(map(sqrt_transfom_x, df_log[col]))\n",
    "    for col in df_log.columns:\n",
    "        df_log[col] = df_log[col].fillna(np.mean(df_log[col]))\n",
    "        sns.distplot(df_log[col], fit=norm)\n",
    "        fig = plt.figure()\n",
    "    return df_log\n",
    "\n",
    "# I will use this function to scale the data with StandardScaler\n",
    "def standard_scaling(df):\n",
    "    transformer = StandardScaler().fit(df)\n",
    "    x_standardized = transformer.transform(df)\n",
    "    X = pd.DataFrame(x_standardized)\n",
    "    X.columns = df.columns\n",
    "    return X\n",
    "\n",
    "# Encoding the categoricals\n",
    "def encode_cat(df):\n",
    "    encoder = OneHotEncoder(drop='first').fit(df)\n",
    "    column_names = encoder.get_feature_names(list(df.columns))\n",
    "    encoded_categorical = encoder.transform(df).toarray()\n",
    "    encoded_categorical = pd.DataFrame(encoded_categorical, columns=column_names)\n",
    "    return encoded_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b296bd",
   "metadata": {},
   "source": [
    "##### We will start with cleaning the datatypes\n",
    "First the dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c655b",
   "metadata": {},
   "source": [
    "#### Cleaning `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc72fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                   21597\n",
      "unique                    372\n",
      "top       2014-06-23 00:00:00\n",
      "freq                      142\n",
      "first     2014-05-02 00:00:00\n",
      "last      2015-05-27 00:00:00\n",
      "Name: date, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of id\n",
       "7129300520   2014-10-13\n",
       "6414100192   2014-12-09\n",
       "5631500400   2015-02-25\n",
       "2487200875   2014-12-09\n",
       "1954400510   2015-02-18\n",
       "                ...    \n",
       "263000018    2014-05-21\n",
       "6600060120   2015-02-23\n",
       "1523300141   2014-06-23\n",
       "291310100    2015-01-16\n",
       "1523300157   2014-10-15\n",
       "Name: date, Length: 21597, dtype: datetime64[ns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "print(data.date.describe())\n",
    "data.date.unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60932acc",
   "metadata": {},
   "source": [
    "We can see that the date the house was sold covers only two years, from 02/05/2014 to 27/05/2015. \\\n",
    "We can bin the date into different groups that have some business sense when it comes to real estate. \\\n",
    "I will therefore group the dates into quarters, which is the best approximation to yearly seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68213bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014Q3    5921\n",
       "2014Q4    4755\n",
       "2015Q1    4100\n",
       "2014Q2    3946\n",
       "2015Q2    2875\n",
       "Freq: Q-DEC, Name: quarter_sold, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['quarter_sold'] = data['date'].dt.to_period('Q')\n",
    "data['quarter_sold'] = data['quarter_sold'].astype(object)\n",
    "\n",
    "# dropping 'date' as 'quarter_sold' is created\n",
    "data = data.drop(['date'], axis=1)\n",
    "\n",
    "data['quarter_sold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fe9ed",
   "metadata": {},
   "source": [
    "#### Cleaning `yr_built`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51be8ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014    559\n",
      "2006    453\n",
      "2005    450\n",
      "2004    433\n",
      "2003    420\n",
      "       ... \n",
      "1933     30\n",
      "1901     29\n",
      "1902     27\n",
      "1935     24\n",
      "1934     21\n",
      "Name: yr_built, Length: 116, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    21597.000000\n",
       "mean      1970.999676\n",
       "std         29.375234\n",
       "min       1900.000000\n",
       "25%       1951.000000\n",
       "50%       1975.000000\n",
       "75%       1997.000000\n",
       "max       2015.000000\n",
       "Name: yr_built, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['yr_built'].value_counts())\n",
    "data['yr_built'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99b32e",
   "metadata": {},
   "source": [
    "We can see that the construction years span 115 years. \\\n",
    "We can group these years by decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8adb4ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000    3515\n",
       "1960    2658\n",
       "1950    2450\n",
       "1970    2285\n",
       "1980    2281\n",
       "1990    2232\n",
       "1940    1763\n",
       "2010    1241\n",
       "1920    1192\n",
       "1910     805\n",
       "1900     645\n",
       "1930     530\n",
       "Name: decade_built, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['decade_built'] = (10 * (data['yr_built'] // 10)).astype(str)\n",
    "\n",
    "#dropping 'yr_built' as 'decade_built' is created\n",
    "data = data.drop(['yr_built'], axis=1)\n",
    "\n",
    "data['decade_built'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e7809",
   "metadata": {},
   "source": [
    "#### Cleaning `yr_renovated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204e5004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       20683\n",
       "2014       91\n",
       "2013       37\n",
       "2003       36\n",
       "2005       35\n",
       "        ...  \n",
       "1951        1\n",
       "1959        1\n",
       "1948        1\n",
       "1954        1\n",
       "1944        1\n",
       "Name: yr_renovated, Length: 70, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10bb3b",
   "metadata": {},
   "source": [
    "`yr_renovated` has a lot of 0 values so we will not change it to datetime but modify the column later turn the data into True or False if the property has been renovated or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043cc2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    20683\n",
       "True       914\n",
       "Name: renovated, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['yr_renovated'] = np.where(data['yr_renovated'] ==0, False, True)\n",
    "data = data.rename(columns={'yr_renovated': 'renovated'})\n",
    "data['renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25030fbb",
   "metadata": {},
   "source": [
    "#### Now we will change the datatypes of the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01161cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "waterfront      object\n",
       "view            object\n",
       "condition       object\n",
       "grade           object\n",
       "zipcode         object\n",
       "renovated       object\n",
       "decade_built    object\n",
       "quarter_sold    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns = ['waterfront', 'view', 'condition', 'grade', 'zipcode', 'renovated', 'decade_built', 'quarter_sold']\n",
    "data[cat_columns] = data[cat_columns].astype(object)\n",
    "data[cat_columns].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5006b94",
   "metadata": {},
   "source": [
    "#### Separating the categorical data, discrete numerical data and continuous numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31355631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_num(df, count):\n",
    "    continuous = []\n",
    "    discrete = []\n",
    "    for col in df:\n",
    "        if len(df[col].value_counts()) > count:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            discrete.append(col)\n",
    "    print(len(continuous), ' continuous numerical columns')\n",
    "    print(len(discrete), 'discrete numerical columns')\n",
    "    continuous_df = df[continuous]\n",
    "    discrete_df = df[discrete]\n",
    "    return continuous_df, discrete_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with only the numerical data for EDA - including `bathrooms`, `floors`, and `bedrooms`\n",
    "numericals =  data.select_dtypes(np.number)\n",
    "\n",
    "# creating a dataframe with only the categorical data\n",
    "cat = data.select_dtypes(np.object)\n",
    "\n",
    "# separeting the discrete and continuous numericals\n",
    "continuous_df, discrete_df = separate_num(numericals, 30)\n",
    "\n",
    "continuous_df.reset_index(drop=True, inplace=True)\n",
    "discrete_df.reset_index(drop=True, inplace=True)\n",
    "print()\n",
    "print(cat)\n",
    "print(continuous_df)\n",
    "print(discrete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45167a6e",
   "metadata": {},
   "source": [
    "### Preliminary model to serve as benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for modeling and model evaluation\n",
    "\n",
    "def modeling(y, X, models=[], test_size=0.3):\n",
    "    for model in models:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        model.fit(X_train, y_train )\n",
    "        predictions = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        r2_adj = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "        RMSE = mean_squared_error(y_test, predictions, squared=False)\n",
    "        MSE = mean_squared_error(y_test, predictions)\n",
    "        MAE = mean_absolute_error(y_test, predictions)\n",
    "        print(model, 'metrics are: '), print(\"R2 =\", round(r2,2)), print(\"R2 adjusted =\", round(r2_adj,2)), print(\"RMSE =\", round(RMSE,2)), print(\"MSE =\", round(MSE,2)), print(\"MAE =\", round(MAE,2))\n",
    "        print()\n",
    "        results = pd.DataFrame()\n",
    "        results['true'] = y_test\n",
    "        results['pred'] = predictions\n",
    "        results['diff'] = results.apply(lambda x: abs(x['true'] - x['pred']), axis=1)\n",
    "        print(results)\n",
    "        print()\n",
    "        beautiful_graph = sns.regplot(x=y_test, y=predictions)\n",
    "        print(beautiful_graph)\n",
    "    return predictions\n",
    "\n",
    "# Create dataframe for visualising the differences between real and predicted values\n",
    "def diff_df(y_test, predictions):\n",
    "    results = pd.DataFrame()\n",
    "    results['true'] = y_test\n",
    "    results['pred'] = predictions\n",
    "    results['diff'] = results.apply(lambda x: abs(x['true'] - x['pred']), axis=1)\n",
    "    results = results.sort_values('diff', ascending=False).head(10)\n",
    "    return results\n",
    "\n",
    "def we_like_to_see(results):\n",
    "    beautiful_graph = sns.regplot(results['true'], results['pred'])\n",
    "    return beautiful_graph\n",
    "\n",
    "def optimal_k(k_min, k_max):\n",
    "    scores = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    for i in range(k_min,k_max): \n",
    "        model = KNeighborsRegressor(n_neighbors=i)\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(k_min, k_max), scores, color = 'blue', linestyle='dashed',\n",
    "    marker='o', markerfacecolor='red', markersize=10)\n",
    "    plt.title('R2 vs. K Value')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = standard_scaling(continuous_df)\n",
    "cat_encoded = pd.get_dummies(cat, columns=cat.columns, drop_first=True)\n",
    "cat_encoded.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e948d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr_beta = modeling(y, X, models=[LinearRegression()], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60813fa",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fde7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions - EDA\n",
    "\n",
    "def check_dist(df):\n",
    "    for column in df:  \n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Quantity')\n",
    "        sns.kdeplot(x=df[column], shade=True)\n",
    "        plt.show()\n",
    "    return ()\n",
    "\n",
    "# Do the same with continuous variables \n",
    "def scatter(df, target):\n",
    "    cols_to_include = df.loc[:, df.columns != target]\n",
    "    for column in cols_to_include:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.scatterplot(df[column],df[target])\n",
    "        plt.show()\n",
    "    return ()\n",
    "\n",
    "def boxplot(df):\n",
    "    for column in df:      \n",
    "        plt.figure(figsize=(6,3))\n",
    "        sns.boxplot(x=df[column])\n",
    "        plt.show()\n",
    "    return ()\n",
    "        \n",
    "def heatmap(df, target):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    matrix = pd.concat([df, target], axis=1)\n",
    "    mask = np.zeros_like(matrix.corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax = sns.heatmap(matrix.corr(), mask=mask, annot=True, cmap='Blues')\n",
    "    m = ax.get_figure()\n",
    "    plt.show()\n",
    "    return m\n",
    "\n",
    "def countplot(df):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.countplot(x=column, data=df, palette='Blues')\n",
    "        plt.show()\n",
    "    return ()\n",
    "\n",
    "# visualize distribution plot with a filter\n",
    "def scatter_x(df, filt, target):\n",
    "    df[filt] = df[filt].astype(int)\n",
    "    unique = list(df[filt].unique())\n",
    "    df_num = df.loc[:, df.columns != target]\n",
    "    df_num = df_num.select_dtypes(np.number) \n",
    "    for col in df_num.columns:\n",
    "        if col != filt:\n",
    "            for i in range(0, len(unique)):\n",
    "                print(filt, ': ', unique[i])\n",
    "                data_element = df_num[df_num[filt]==unique[i]]\n",
    "                plt.figure(figsize=(10,5))\n",
    "                sns.scatterplot(data_element[col], df[target])\n",
    "                plt.show()\n",
    "\n",
    "# visualize the box-plots with a filter\n",
    "def boxplot_x(df, filt):\n",
    "    df[filt] = df[filt].astype(int)\n",
    "    unique = list(df[filt].unique())\n",
    "    df_num = df.select_dtypes(np.number) \n",
    "    for col in df_num.columns:\n",
    "        if col != filt:\n",
    "            for i in range(0, len(unique)):\n",
    "                print(filt, ': ', unique[i])\n",
    "                data_element = df_num[df_num[filt]==unique[i]]\n",
    "                plt.figure(figsize=(10,5))\n",
    "                sns.boxplot(data_element[col])\n",
    "                plt.show()\n",
    "                \n",
    "# vizualizing variables repartition on a map\n",
    "def mapping(df, variables):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    for variable in variables:\n",
    "        df.plot(kind='scatter', x='long', y='lat', alpha=0.4, figsize=(14,11),\n",
    "        c=variable, cmap=plt.get_cmap('turbo'), colorbar=True, sharex=False)\n",
    "        plt.title(f'{variable} distribution')\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ec854",
   "metadata": {},
   "source": [
    "##### Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe111d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distributions = check_dist(continuous_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8441b",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Most of the distributions resemble skewed normal distributions except for `sqft_lot` and `sqft_lot15` as their values hover around one value, and `sqft_basement` which has most of its value around 0 and then a second flatter curve for the properties that have a basement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07139d77",
   "metadata": {},
   "source": [
    "##### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d0c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatterplots = scatter(continuous_df, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94615113",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "From looking at the scatterplots, we can see that there is a clear linear correlation between the `sqft_living`, `sqft_above`, `sqft_living15`, `sqft_basement` and `price`. For the latter, there are also more expensive properties that don't have a basement hence the continuous line on `0` `sqft_basement`.\n",
    "\n",
    "Additionaly, it looks like `sqf_lot` and `sqft_lot15` have very low correlation with the `price`, there are very high prices for very low `sqf_lot` values and very low prices for high and low `sqf_lot` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfa7e0",
   "metadata": {},
   "source": [
    "##### Box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaba4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots = boxplot(numericals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b322a",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "There are a lot of outliers, however, given the many different features of the properties what looks like outliers in one feature might be explained by another feature and be perfectly within the norm within a subcategory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd953718",
   "metadata": {},
   "source": [
    "##### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = heatmap(numericals.drop(['price'], axis=1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d2661",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The heatmap shows that `sqft_living` and `sqft_above` are very colinear. \\\n",
    "We could consider engineering a new feature out of the two or dropping the one that is the most correlated with the target `price`.\n",
    "\n",
    "The variables the most correlated to the target are `sqft_living`, `sqft_above`, `sqft_living_15` and `bathrooms`.\n",
    "The variables the least correlated to the target are `long`, `sqft_lot`, and `sqft_lot15`. \\\n",
    "Interestingly, `lat` is much more correlated to the target than `long`, so the property location along the North-South axe has more impact on the `price` than the location along the East-West axe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648971a",
   "metadata": {},
   "source": [
    "##### Countplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49db6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countplots = countplot(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fab4e",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The variables `waterfront`, `view`, `renovated` have one dominant value and `condition` has very few `1` and `2` compared to the other three values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b440d",
   "metadata": {},
   "source": [
    "### Exploring the price distribution per the following categorical and discrete features\n",
    "First per `zipcodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d9123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zipcodes = scatter_x(data, 'zipcode', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13861d62",
   "metadata": {},
   "source": [
    "Per `renovated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e222b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "renovated_and_not = scatter_x(data, 'renovated', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cbb86",
   "metadata": {},
   "source": [
    "Per `view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13db63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "views = scatter_x(data, 'view', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cc7bc",
   "metadata": {},
   "source": [
    "Per `waterfront`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de46e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waterfront_or_not = scatter_x(data, 'waterfront', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bafb006",
   "metadata": {},
   "source": [
    "Per `condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cd5d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conditions = scatter_x(data, 'condition', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996e035",
   "metadata": {},
   "source": [
    "Per `grade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a6b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grades = scatter_x(data, 'grade', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9625ac",
   "metadata": {},
   "source": [
    "Per `decade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd47ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decades_built = scatter_x(data, 'decade_built', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d126d6",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The distribution plots show that `zipcode`, `grade`, and `condition` have the highest impact on the features of the properties and price distribution, `decade_built` and `view` also affect them albeit to a lesser extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd166bb0",
   "metadata": {},
   "source": [
    "### Exploring the outlier frequency per grade, zipcode, and condition\n",
    "First per `grade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b43795",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_box = boxplot_x(data, 'grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce9cfb",
   "metadata": {},
   "source": [
    "We can see that the higher the grade, the higher the number of outliers.\n",
    "\n",
    "Per `zipcode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2862dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_box = boxplot_x(data, 'zipcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb1ed3",
   "metadata": {},
   "source": [
    "Looking at the outliers per zipcodes highlights the significant differences between zipcodes.\n",
    "\n",
    "Per `condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_box = boxplot_x(data, 'condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786305c",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Overall, the `sqft` features and the `price` are the most affected by the different values of the features `grade`, `zipcode` and `condition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb667179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing long and lat\n",
    "ft = list(numericals.columns)\n",
    "ft.remove('long')\n",
    "ft.remove('lat')\n",
    "ft.extend(['grade', 'waterfront', 'condition', 'view', 'decade_built', 'zipcode'])\n",
    "\n",
    "# mapping the variables\n",
    "maps = mapping(data, ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dffd23",
   "metadata": {},
   "source": [
    "We can see that the most expensive properties are stituated in the center-north of the map, next to the water (slightly above 47.6 and to the left of -122.2).\n",
    "\n",
    "**From the other maps, it is noticeable that this expensive area overlaps with**:\n",
    "\n",
    "        - more `bathrooms` \n",
    "        - higher `sqft_living` and `sqft_above`\n",
    "        - mostly `2` or less `floors`\n",
    "        - slightly more `sqft_basement`\n",
    "        - bigger living room area\n",
    "        - higher `grade` (`10`+)\n",
    "        - not necessarily better `condition`, it seems to be averaging around `3` to `4`\n",
    "        - the decades the properties were built ranges between `1950` and `2010` albeit most were built before `2000`.\n",
    "        - this expensive area overlaps two `zipcode`\n",
    "\n",
    "**These features are not parameters that influence very high** `price`:\n",
    "\n",
    "        - `view` always rates better next to the water, it's not exclusive to this expensive area. \n",
    "           Other areas with better views also have higher a higher `price` than average but not to the extent of the zone with the most expensive properties.\n",
    "        - the 'lot' variables, `sqft_lot` and `sqft_lot15` have higher values on the outer edges of the map and have no overlap with the property area with the highest `price`.\n",
    "          We can infer that it's due to the urban nature of the zone, where most of the square footage is indoors.      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5fbba",
   "metadata": {},
   "source": [
    "## Feature Engineering and Feature Selection\n",
    "\n",
    "#### Binning `lat` and dropping `long`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c89a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique `lat` values: ', len(continuous_df['lat'].unique()), '\\n')\n",
    "print('Value repartition: ')\n",
    "continuous_df['lat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy\n",
    "df_lat = continuous_df.copy()\n",
    "\n",
    "# binning lat\n",
    "step = 0.001\n",
    "lat_bin = lambda x: np.floor(x / step) * step\n",
    "\n",
    "# creating a new column with the bins\n",
    "df_lat['lat_bin'] = df_lat['lat'].map(lat_bin)\n",
    "\n",
    "# checking the values\n",
    "print(len(df_lat['lat_bin'].unique()))\n",
    "df_lat['lat_bin'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping long and lat\n",
    "df_lat_ = df_lat.drop(['long', 'lat'], axis=1)\n",
    "\n",
    "# scaling the dataframe copy\n",
    "X_lat_scaled = standard_scaling(df_lat_)\n",
    "\n",
    "# concatenating with the other dataframes and dropping the target\n",
    "X_lat = pd.concat([X_lat_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X_lat.drop(['price'], axis=1, inplace=True)\n",
    "\n",
    "# checking the model's performance\n",
    "evol_1 = modeling(y, X_lat, models=[LinearRegression()], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2246e3",
   "metadata": {},
   "source": [
    "There is almost no difference in the model's performance, only the MAE increased very little.\n",
    "\n",
    "#### Dropping `lat` and  `long`\n",
    "`long` is very little correlated to the target and we have already a more precise geographical parameter with `zipcode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20832d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping lat and long\n",
    "continuous_df.drop(['lat', 'long'], axis=1, inplace=True)\n",
    "data.drop(['lat', 'long'], axis=1, inplace=True)\n",
    "\n",
    "# scaling the dataframe copy\n",
    "X_scaled = standard_scaling(continuous_df)\n",
    "\n",
    "# concatenating with the other dataframes and dropping the target\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X.drop(['price'], axis=1, inplace=True)\n",
    "\n",
    "# checking the model's performance\n",
    "evol_2 = modeling(y, X, models=[LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are mostly the same, only the MAE improved by very little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187867ca",
   "metadata": {},
   "source": [
    "#### Looking at the difference between `sqft_lot` and  `sqft_lot15`\n",
    "The heatmap showed that these two features have very little correlation with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('corrlation of sqft_lot and sqft_lot15 witht the target:\\n', \n",
    "      continuous_df.corrwith(continuous_df['price']).sort_values(ascending=False).head(20), '\\n')\n",
    "print('do not show in the top :\\n', X.corrwith(continuous_df['price']).sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3efd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_lot_15 = pd.DataFrame()\n",
    "diff_lot_15['lot_15'] = continuous_df['sqft_lot15']\n",
    "diff_lot_15['lot'] = continuous_df['sqft_lot']\n",
    "diff_lot_15['diff'] = diff_lot_15.apply(lambda x: x['lot_15'] - x['lot'], axis=1)\n",
    "diff_lot_15 = diff_lot_15.sort_values('diff', ascending=False)\n",
    "print('the diff median is: ', diff_lot_15['diff'].median())\n",
    "print()\n",
    "print(diff_lot_15['diff'].describe())\n",
    "print()\n",
    "print(diff_lot_15.head(5))\n",
    "print()\n",
    "print(diff_lot_15.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb873cd",
   "metadata": {},
   "source": [
    "We could create a new column with the lot evolution and drop `sqft_lot` and `sqft_lot15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_df['lot_evolution'] = continuous_df.apply(lambda x: x['sqft_lot15'] - x['sqft_lot'], axis=1)\n",
    "data['lot_evolution'] = data.apply(lambda x: x['sqft_lot15'] - x['sqft_lot'], axis=1)\n",
    "\n",
    "print('corrlation of sqft_lot and sqft_lot15 witht the target:\\n', \n",
    "      continuous_df.corrwith(continuous_df['price']).sort_values(ascending=False).head(20), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping sqft_lot and sqft_lot15\n",
    "continuous_df = continuous_df.drop(['sqft_lot', 'sqft_lot15'], axis=1)\n",
    "data = data.drop(['sqft_lot', 'sqft_lot15'], axis=1)\n",
    "\n",
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(continuous_df)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']\n",
    "\n",
    "# checking the model's performance\n",
    "evol_2 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are mostly the same, only the MAE improved by very little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fa5d2",
   "metadata": {},
   "source": [
    "### Removing the outliers\n",
    "\n",
    "Some outliers in the complete dataset are actually features of thir own group. \n",
    "\n",
    "I will try two methods:\n",
    "\n",
    "        - removing the outliers per `zipcode` to preserve geographical specificities\n",
    "        - removing the outliers per `grade` to preserve the insight of real estate business experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d11cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-assigning the correct datatype to the categorical data\n",
    "cat_columns_bis = ['waterfront', 'view', 'condition', 'grade', 'zipcode', 'renovated', 'decade_built', 'quarter_sold']\n",
    "data[cat_columns_bis] = data[cat_columns_bis].astype(object)\n",
    "data[cat_columns_bis].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f60e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the outliers per a filter column to keep specifities\n",
    "def outliers(df, filt, target):\n",
    "    df[filt] = df[filt].astype(int)\n",
    "    unique = list(df[filt].unique())\n",
    "    \n",
    "    df_num = df.loc[:, df.columns != target]\n",
    "    df_num = df_num.select_dtypes(np.number)\n",
    "    \n",
    "    out = pd.DataFrame(columns=df_num.columns)\n",
    "    \n",
    "    for i in range(0, len(unique)):\n",
    "        data_element = df_num[df_num[filt]==unique[i]]\n",
    "        for col in data_element.columns:\n",
    "            \n",
    "            if col != filt:\n",
    "                upper = np.percentile(data_element[col], 75)\n",
    "                lower = np.percentile(data_element[col], 25)\n",
    "                iqr = upper - lower\n",
    "                upper_limit = upper + 1.5 * iqr\n",
    "                lower_limit = lower - 1.5 * iqr\n",
    "                \n",
    "                no_outliers = data_element[(data_element[col]>lower_limit) & (data_element[col]<upper_limit)]\n",
    "                \n",
    "        out = pd.concat([out, no_outliers])\n",
    "    \n",
    "    out = out.drop([filt], axis=1)\n",
    "    #data = pd.merge(out, cat, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68554be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_lot = data.drop(['lot_evolution'], axis=1)\n",
    "data_no_lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the numerical data with the outliers removed per 'grade' values\n",
    "num_grades = outliers(data_no_lot, 'grade', 'price')\n",
    "num_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the numerical data with the outliers removed per 'zipcode' values\n",
    "num_zipcodes = outliers(data_no_lot, 'zipcode', 'price')\n",
    "num_zipcodes = num_zipcodes.drop(['grade'], axis=1)\n",
    "num_zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0c54a",
   "metadata": {},
   "source": [
    "#### Modeling with the outliers removed by `grade` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the discrete numericals with the target to have the same numbers of rows\n",
    "discrete_df_grades = num_grades[['bedrooms', 'bathrooms', 'floors']]\n",
    "target = data['price']\n",
    "target = target.to_frame()\n",
    "discrete_df_grades = discrete_df_grades.merge(target, how='left', left_index=True, right_index=True)\n",
    "discrete_df_grades.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# re-creating the continuous numericals dataframe\n",
    "continuous_df_grades = num_grades.drop(['bedrooms', 'bathrooms', 'floors'], axis=1)\n",
    "continuous_df_grades.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(continuous_df_grades)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "nums = pd.concat([discrete_df_grades,X_scaled], axis=1)\n",
    "X = pd.merge(nums, cat_encoded, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# isolating the target and dropping it from our explanatory variable dataset\n",
    "y = X['price']\n",
    "X = X.drop(['price'], axis=1)\n",
    "\n",
    "# checking the model's performance\n",
    "evol_3 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the model does not perform as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c44e44",
   "metadata": {},
   "source": [
    "#### Modeling with the outliers removed by `zipcode` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the discrete numericals with the target to have the same numbers of rows\n",
    "discrete_df_grades = num_zipcodes[['bedrooms', 'bathrooms', 'floors']]\n",
    "target = data['price']\n",
    "target = target.to_frame()\n",
    "discrete_df_grades = discrete_df_grades.merge(target, how='left', left_index=True, right_index=True)\n",
    "discrete_df_grades.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# re-creating the continuous numericals dataframe\n",
    "continuous_df_grades = num_zipcodes.drop(['bedrooms', 'bathrooms', 'floors'], axis=1)\n",
    "continuous_df_grades.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(continuous_df_grades)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "nums = pd.concat([discrete_df_grades,X_scaled], axis=1)\n",
    "X = pd.merge(nums, cat_encoded, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# isolating the target and dropping it from our explanatory variable dataset\n",
    "y = X['price']\n",
    "X = X.drop(['price'], axis=1)\n",
    "\n",
    "# checking the model's performance\n",
    "evol_3 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are a lot worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fca842",
   "metadata": {},
   "source": [
    "#### Checking if applying a transformation to improve the distribution helps the model perform better\n",
    "\n",
    "###### Log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f58f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_numericals = log_transform_df(continuous_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77578c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(log_numericals)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']\n",
    "\n",
    "# checking the model's performance\n",
    "evol_4 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are not as good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06983b6",
   "metadata": {},
   "source": [
    "###### Square-root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ceb159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqrt_numericals = sqrt_transform_df(continuous_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22828827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(sqrt_numericals)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']\n",
    "\n",
    "# checking the model's performance\n",
    "evol_5 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are mostly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fe3d5",
   "metadata": {},
   "source": [
    "### Houses valued at 650K and above\n",
    "\n",
    "#### Understanding the factors that are responsible for higher property value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rich = data.loc[data['price']>650000]\n",
    "print(df_rich.shape)\n",
    "df_rich.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96295bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a37fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127e3a2c",
   "metadata": {},
   "source": [
    "## Comparing regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501346cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(continuous_df)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6291cd",
   "metadata": {},
   "source": [
    "### Finding the best k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fef954",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = optimal_k(2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea35abb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_4m = modeling(y, X, models=[LinearRegression(), KNeighborsRegressor(n_neighbors=7), RandomForestRegressor(max_depth=10), GradientBoostingRegressor()], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a8ad5",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning with RandomForestRegressor and GradientBoostingRegressor\n",
    "\n",
    "#### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c056d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "gb_hyperparameters = {\n",
    "    'n_estimators' : [350, 450],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2],\n",
    "    'min_samples_split': [2, 3, 4],    \n",
    "    'max_depth' : [4, 5, 7]}\n",
    "\n",
    "\n",
    "# Grid search - GradientBoostingRegressor\n",
    "grid_search_gb = GridSearchCV(gbr, gb_hyperparameters, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True, n_jobs=-1)\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "gbr_best_parameters = grid_search_gb.best_params_\n",
    "print(gbr_best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3af269",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_best_parameters = grid_search_gb.best_params_\n",
    "gbr_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators = 450, max_depth=4, min_samples_split=4, learning_rate=0.1, random_state=42)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "gbr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9591cb1",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the range of hyper-parameters:\n",
    "rf_hyperparameters = {\n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'n_estimators' : [50, 100],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'max_features' : ['auto', 'sqrt'],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [3,5],\n",
    "    'min_samples_leaf' : [1, 3, 5]}\n",
    "\n",
    "# Grid search - RandomForestRegressor\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), rf_hyperparameters, cv=5,\n",
    "                           scoring='neg_root_mean_squared_error',\n",
    "                           return_train_score=True, n_jobs=-1)\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(grid_search_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c32dfc",
   "metadata": {},
   "source": [
    "### Making pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cfd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "pipelines = {\n",
    "    'rf' :    make_pipeline(RandomForestRegressor(random_state=42)),\n",
    "    'gb' :    make_pipeline(GradientBoostingRegressor(random_state=42))\n",
    "}\n",
    "\n",
    "rf_hyperparameters = {\n",
    "     'randomforestregressor__n_estimators' : [100, 500],\n",
    "     'randomforestregressor__max_features' : ['auto', 'sqrt'],\n",
    "     'randomforestregressor__min_samples_leaf' : [1, 3, 5, 7]}\n",
    "\n",
    "gb_hyperparameters = {\n",
    "      'gradientboostingregressor__n_estimators' : [50, 200],\n",
    "      'gradientboostingregressor__learning_rate' : [0.05, 0.1, 0.2],\n",
    "      'gradientboostingregressor__max_depth' : [4, 5, 6,7,8]}\n",
    "\n",
    "grid_search = GridSearchCV(model, )\n",
    "\n",
    "\n",
    "fitted_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    model = GridSearchCV(pipeline, \n",
    "                         hyperparameters[name], \n",
    "                         cv=10, \n",
    "                         n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted_models[name] = model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
