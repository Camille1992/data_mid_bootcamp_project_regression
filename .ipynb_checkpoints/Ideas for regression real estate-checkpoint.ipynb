{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01631962",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.2\n",
    "to_bin = lambda x: np.floor(x / step) * step\n",
    "df[\"latBin\"] = to_bin(df.Latitude)\n",
    "df[\"lonBin\"] = to_bin(df.Longitude)\n",
    "groups = df.groupby([\"latBin\", \"lonBin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning `bathrooms` - I will keep 0 for the 0.5 and 0.75 as just a sink and a toilet is quite different from only a shower = 1 bathroom\n",
    "to_bin = lambda x: np.floor(x) \n",
    "\n",
    "data['bathrooms'] = to_bin(data['bathrooms'] )\n",
    "\n",
    "data.bathrooms.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_num(df, count):\n",
    "    continuous = []\n",
    "    discrete = []\n",
    "    for col in df:\n",
    "        if len(df[col].value_counts()) > count:\n",
    "            continuous.append(col)\n",
    "        else:\n",
    "            discrete.append(col)\n",
    "    print(len(continuous), ' continuous numerical columns')\n",
    "    print(len(discrete), 'discrete numerical columns')\n",
    "    continuous_df = df[continuous]\n",
    "    discrete_df = df[discrete]\n",
    "    return continuous_df, discrete_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(continuous_df)\n",
    "\n",
    "# encoding the categorical data\n",
    "cat_encoded = pd.get_dummies(cat, columns=cat.columns, drop_first=True)\n",
    "\n",
    "# resetting the index\n",
    "cat_encoded.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "print(data.date.describe())\n",
    "data.date.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('corrlation of sqft_lot and sqft_lot15 witht the target:\\n', \n",
    "      continuous_df.corrwith(continuous_df['price']).sort_values(ascending=False).head(20), '\\n')\n",
    "print('do not show in the top :\\n', X.corrwith(continuous_df['price']).sort_values(ascending=False).head(20))\n",
    "\n",
    "# dropping sqft_lot and sqft_lot15\n",
    "continuous_df.drop(['sqft_lot', 'sqft_lot15'], axis=1, inplace=True)\n",
    "data.drop(['sqft_lot', 'sqft_lot15'], axis=1, inplace=True)\n",
    "X.drop(['sqft_lot', 'sqft_lot15'], axis=1, inplace=True)\n",
    "\n",
    "# checking the model's performance\n",
    "evol_2 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are mostly the same, only the MAE improved by very little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = continuous_df.drop(['sqft_living'], axis=1)\n",
    "#cat_test = cat.drop(['waterfront'], axis=1)\n",
    "dis_test = discrete_df.drop(['bedrooms'], axis=1)\n",
    "dis_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# scaling the data with StandardScaler\n",
    "X_scaled = standard_scaling(test)\n",
    "\n",
    "# encoding the categorical data\n",
    "cat_encoded = pd.get_dummies(cat_test, columns=cat_test.columns, drop_first=True)\n",
    "cat_encoded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concatenating the dataframes to create our exogenous dataset\n",
    "X = pd.concat([X_scaled, discrete_df, cat_encoded], axis=1)\n",
    "X = X.drop('price', axis=1)\n",
    "\n",
    "# isolating the target\n",
    "y = data['price']\n",
    "\n",
    "# checking the model's performance\n",
    "evol_3 = modeling(y, X, models = [LinearRegression()], test_size=0.3)\n",
    "\n",
    "# the results are mostly the same, only the MAE improved by very little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = continuous_df.copy()\n",
    "\n",
    "\n",
    "test['sqft_basement'] = np.where(test['sqft_basement'] ==0, 'No', 'Yes')\n",
    "test = test.rename(columns={'sqft_basement': 'basement'})\n",
    "test['basement'].value_counts()\n",
    "\n",
    "s = test['basement']\n",
    "basement = s.to_frame()\n",
    "basement.head()\n",
    "\n",
    "# dropping yr_built as decade_built was created\n",
    "cat = categoricals.drop(['yr_built'], axis=1)\n",
    "cat.reset_index(drop=True, inplace=True)\n",
    "basement.reset_index(drop=True, inplace=True)\n",
    "cat = pd.concat([cat, basement], axis=1)\n",
    "cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b324996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "pipelines = {\n",
    "    'rf' :    make_pipeline(RandomForestRegressor(random_state=42)),\n",
    "    'gb' :    make_pipeline(GradientBoostingRegressor(random_state=42))\n",
    "}\n",
    "\n",
    "rf_hyperparameters = {\n",
    "     'randomforestregressor__n_estimators' : [100, 500],\n",
    "     'randomforestregressor__max_features' : ['auto', 'sqrt'],\n",
    "     'randomforestregressor__min_samples_leaf' : [1, 3, 5, 7]}\n",
    "\n",
    "gb_hyperparameters = {\n",
    "      'gradientboostingregressor__n_estimators' : [50, 200],\n",
    "      'gradientboostingregressor__learning_rate' : [0.05, 0.1, 0.2],\n",
    "      'gradientboostingregressor__max_depth' : [4, 5, 6,7,8]}\n",
    "\n",
    "grid_search = GridSearchCV(model, )\n",
    "\n",
    "\n",
    "fitted_models = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    model = GridSearchCV(pipeline, \n",
    "                         hyperparameters[name], \n",
    "                         cv=10, \n",
    "                         n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted_models[name] = model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
